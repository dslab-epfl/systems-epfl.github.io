<table class="table">
<tbody>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>ISCA'21</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/285466">Rebooting Virtual Memory with Midgard</a>.
			</span><br />
			<span class="authors">
				Siddharth Gupta, Atri Bhattacharyya, Yunho Oh, Abhishek Bhattacharjee, Babak Falsafi, and Mathias Payer.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Gupta:285466" class="abstract">
			Computer systems designers are building cache hierarchies with higher capacity to capture the ever-increasing working sets of modern workloads. Cache hierarchies with higher capacity improve system performance but shift the performance bottleneck to address translation. We propose Midgard, an intermediate address space between the virtual and the physical address spaces, to mitigate address translation overheads without program-level changes. Midgard leverages the operating system concept of virtual memory areas (VMAs) to realize a single Midgard address space where VMAs of all processes can be uniquely mapped. The Midgard address space serves as the namespace for all data in a coherence domain and the cache hierarchy. Because real-world workloads use far fewer VMAs than pages to represent their virtual address space, virtual to Midgard translation is achieved with hardware structures that are much smaller than TLB hierarchies. Costlier Midgard to physical address translations are needed only on LLC misses, which become much less frequent with larger caches. As a consequence, Midgard shows that instead of amplifying address translation overheads, memory hierarchies with large caches can reduce address translation overheads. Our evaluation shows that Midgard achieves only 5% higher address translation overhead as compared to traditional TLB hierarchies for 4KB pages when using a 16MB aggregate LLC. Midgard also breaks even with traditional TLB hierarchies for 2MB pages when using a 256MB aggregate LLC. For cache hierarchies with higher capacity, Midgard&#39;s address translation overhead drops to near zero as secondary and tertiary data working sets fit in the LLC, while traditional TLBs suffer even higher degrees of address translation overhead.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>MICRO'21</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/288432">Cerebros: Evading the RPC Tax in Datacenters</a>.
			</span><br />
			<span class="authors">
				Arash Pourhabibi Zarandi, Mark Johnathon Sutherland, Alexandros Daglis, and Babak Falsafi.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_PourhabibiZarandi:288432" class="abstract">
			The emerging paradigm of microservices decomposes online services into fine-grained software modules frequently communicating over the datacenter network, often using Remote Procedure Calls (RPCs). Ongoing advancements in the network stack have exposed the RPC layer itself as a bottleneck, that we show accounts for 40‚Äì90% of a microservice&#39;s total execution cycles. We break down the underlying modules that comprise production RPC layers and demonstrate, based on prior evidence, that CPUs can only expect limited improvements for such tasks, mandating a shift to hardware to remove the RPC layer as a limiter of microservice performance. Although recently proposed accelerators can efficiently handle a portion of the RPC layer, their overall benefit is limited by unnecessary CPU involvement, which occurs because the accelerators are architected as co-processors under the CPU&#39;s control. Instead, we show that conclusively removing the RPC layer bottleneck requires all of the RPC layer&#39;s modules to be executed by a NIC-attached hardware accelerator. We introduce Cerebros, a dedicated RPC processor that executes the Apache Thrift RPC layer and acts as an intermediary stage between the NIC and the microservice running on the CPU. Our evaluation using the DeathStarBench microservice suite shows that Cerebros reduces the CPU cycles spent in the RPC layer by 37‚Äì64√ó, yielding a 1.8‚Äì14√ó reduction in total cycles expended per microservice request.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>MICRO'21</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/288715">Equinox: Training (for Free) on a Custom Inference Accelerator</a>.
			</span><br />
			<span class="authors">
				Mario Paulo Drumond Lages De Oliveira, Louis Coulon, Arash Pourhabibi Zarandi, Ahmet Caner Y√ºz√ºg√ºler, Babak Falsafi, and Martin Jaggi.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_DrumondLagesDeOliveira:288715" class="abstract">
			DNN inference accelerators executing online services exhibit low average loads because of service demand variability, leading to poor resource utilization. Unfortunately, reclaiming idle inference cycles is difficult as other workloads can not execute on a custom accelerator. With recent proposals for the use of fixed-point arithmetic in training, there are opportunities for training services to piggyback on inference accelerators. We make the observation that a key challenge in doing so is maintaining service-level latency constraints for inference. We show that relaxing latency constraints in an inference accelerator with ALU arrays that are batching-optimized achieves near-optimal throughput for a given area and power envelope while maintaining inference services&#39; tail latency goals. We present Equinox, a custom inference accelerator designed to piggyback training. Equinox employs a uniform arithmetic encoding to accommodate inference and training and a priority hardware scheduler with adaptive batching that interleaves training during idle inference cycles. For a500ùúáùë† inference service time constraint, Equinox achieves 6.67√ó higher throughput than a latency-optimal inference accelerator. Despite not being optimized for training services, Equinox achieves up to 78% of the throughput of a dedicated training accelerator that saturates the available compute resources and DRAM bandwidth. Finally, Equinox‚Äôs controller logic incurs less than 1% power and area overhead, while the uniform encoding (to enable training) incurs 13% power and 4% area overhead compared to a fixed-point inference accelerator.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>OSDI'21</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/289218">NrOS: Effective Replication and Sharing in an Operating System</a>.
			</span><br />
			<span class="authors">
				Ankit Bhardwaj, Chinmay Kulkarni, Reto Achermann, Irina Calciu, Sanidhya Kashyap, Ryan Stutsman, Amy Tai, and Gerd Zellweger.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Bhardwaj:289218" class="abstract">
			Writing a correct operating system kernel is notoriously hard. Kernel code requires manual memory management and type-unsafe code and must efficiently handle complex, asynchronous events. In addition, increasing CPU core counts further complicate kernel development. Typically, monolithic kernels share state across cores and rely on one-off synchronization patterns that are specialized for each kernel structure or subsystem. Hence, kernel developers are constantly refining synchronization within OS kernels to improve scalability at the risk of introducing subtle bugs.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>SIGMOD'21</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/285140">Scalable Multi-Query Execution using Reinforcement Learning</a>.
			</span><br />
			<span class="authors">
				Panagiotis Sioulas and Anastasia Ailamaki.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Sioulas:285140" class="abstract">
			The growing demand for data-intensive decision support and the migration to multi-tenant infrastructures put databases under the stress of high analytical query load. The requirement for high throughput contradicts the traditional design of query-at-a-time databases that optimize queries for efficient serial execution. Sharing work across queries presents an opportunity to reduce the total cost of processing and therefore improve throughput with increasing query load. Systems can share work either by assessing all opportunities and restructuring batches of queries ahead of execution, or by inspecting opportunities in individual incoming queries at runtime: the former strategy scales poorly to large query counts, as it requires expensive sharing-aware optimization, whereas the latter detects only a subset of the opportunities. Both strategies fail to minimize the cost of processing for large and ad-hoc workloads. This paper presents RouLette, a specialized intelligent engine for multi-query execution that addresses, through runtime adaptation, the shortcomings of existing work-sharing strategies. RouLette scales by replacing sharing-aware optimization with adaptive query processing, and it chooses opportunities to explore and exploit by using reinforcement learning. RouLette also includes optimizations that reduce the adaptation overhead. RouLette increases throughput by 1.6-28.3x, compared to a state-of-the-art query-at-a-time engine, and up to 6.5x, compared to sharing-enabled prototypes, for multi-query workloads based on the schema of TPC-DS.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>VLDB End.'21</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/287444">CALYPSO: Private Data Management for Decentralized Ledgers</a>.
			</span><br />
			<span class="authors">
				Eleftherios Kokoris Kogias, Enis Ceyhun Alp, Linus Gasser, Philipp Svetolik Jovanovic, Ewa Syta, and Bryan Alexander Ford.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_KokorisKogias:287444" class="abstract">
			Distributed ledgers provide high availability and integrity, making them a key enabler for practical and secure computation of distributed workloads among mutually distrustful parties. Many practical applications also require strong confidentiality, however. This work enhances permissioned and permissionless blockchains with the ability to manage confidential data without forfeiting availability or decentralization. The proposed Calypso architecture addresses two orthogonal challenges confronting modern distributed ledgers: (a) enabling the auditable management of secrets and (b) protecting distributed computations against arbitrage attacks when their results depend on the ordering and secrecy of inputs. Calypso introduces on-chain secrets, a novel abstraction that enforces atomic deposition of an auditable trace whenever users access confidential data. Calypso provides user-controlled consent management that ensures revocation atomicity and accountable anonymity. To enable permissionless deployment, we introduce an incentive scheme and provide users with the option to select their preferred trustees. We evaluated our Calypso prototype with a confidential document-sharing application and a decentralized lottery. Our benchmarks show that transaction-processing latency increases linearly in terms of security (number of trustees) and is in the range of 0.2 to 8 seconds for 16 to 128 trustees.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>VLDB'21</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/284817">Micro-architectural Analysis of In-memory OLTP: Revisited</a>.
			</span><br />
			<span class="authors">
				Utku Sirin, Pƒ±nar T√∂z√ºn, Danica Porobic, Ahmad Yasin, and Anastasia Ailamaki.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Sirin:284817" class="abstract">
			Micro-architectural behavior of traditional disk-based online transaction processing (OLTP) systems has been investigated extensively over the past couple of decades. Results show that traditional OLTP systems mostly under-utilize the available micro-architectural resources. In-memory OLTP systems, on the other hand, process all the data in main-memory and, therefore, can omit the buffer pool. Furthermore, they usually adopt more lightweight concurrency control mechanisms, cache-conscious data structures, and cleaner codebases since they are usually designed from scratch. Hence, we expect significant differences in micro-architectural behavior when running OLTP on platforms optimized for in-memory processing as opposed to disk-based database systems. In particular, we expect that in-memory systems exploit micro-architectural features such as instruction and data caches significantly better than disk-based systems. This paper sheds light on the micro-architectural behavior of in-memory database systems by analyzing and contrasting it to the behavior of disk-based systems when running OLTP workloads. The results show that, despite all the design changes, in-memory OLTP exhibits very similar micro-architectural behavior to disk-based OLTP: more than half of the execution time goes to memory stalls where instruction cache misses or the long-latency data misses from the last-level cache (LLC) are the dominant factors in the overall execution time. Even though ground-up designed in-memory systems can eliminate the instruction cache misses, the reduction in instruction stalls amplifies the impact of LLC data misses. As a result, only 30% of the CPU cycles are used to retire instructions, and 70% of the CPU cycles are wasted to stalls for both traditional disk-based and new generation in-memory OLTP.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>ASPLOS'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/274129">Optimus Prime: Accelerating Data Transformation in Servers</a>.
			</span><br />
			<span class="authors">
				Arash Pourhabibi Zarandi, Siddharth Gupta, Hussein Kassir, Mark Johnathon Sutherland, Zilu Tian, Mario Paulo Drumond Lages De Oliveira, Babak Falsafi, and Christoph Koch.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_PourhabibiZarandi:274129" class="abstract">
			Modern online services are shifting away from monolithic applications to loosely-coupled microservices because of their improved scalability, reliability, programmability and development velocity. Microservices communicating over the datacenter network require data transformation (DT) to convert messages back and forth between their internal formats. This work identifies DT as a bottleneck due to reductions in latency of the surrounding system components, namely application runtimes, protocol stacks, and network hardware. We therefore propose Optimus Prime (OP), a programmable DT accelerator that uses a novel abstraction, an in-memory schema, to represent DT operations. The schema is compatible with today&#39;s DT frameworks and enables any compliant accelerator to perform the transformations comprising a request in parallel. Our evaluation shows that OP&#39;s DT throughput matches the line rate of today&#39;s NICs and has 60x higher throughput compared to software, at a tiny fraction of the CPU&#39;s silicon area and power. We also evaluate a set of microservices running on Thrift, and show up to 30% reduction in service latency.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>OSDI'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/287833">Microsecond Consensus for Microsecond Applications</a>.
			</span><br />
			<span class="authors">
				Marcos K. Aguilera, Naama Ben-David, Rachid Guerraoui, Virendra J. Marathe, Athanasios Xygkis, and Igor Zablotchi.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Aguilera:287833" class="abstract">
			We consider the problem of making apps fault-tolerant through replication, when apps operate at the microsecond scale, as in finance, embedded computing, and microservices apps. These apps need a replication scheme that also operates at the microsecond scale, otherwise replication becomes a burden. We propose Mu, a system that takes less than 1.3 microseconds to replicate a (small) request in memory, and less than a millisecond to fail-over the system-this cuts the replication and fail-over latencies of the prior systems by at least 61% and 90%. Mu implements bona fide state machine replication/consensus (SMR) with strong consistency for a generic app, but it really shines on microsecond apps, where even the smallest overhead is significant. To provide this performance, Mu introduces a new SMR protocol that carefully leverages RDMA. Roughly, in Mu a leader replicates a request by simply writing it directly to the log of other replicas using RDMA, without any additional communication. Doing so, however, introduces the challenge of handling concurrent leaders, changing leaders, garbage collecting the logs, and more-challenges that we address in this paper through a judicious combination of RDMA permissions and distributed algorithmic design. We implemented Mu and used it to replicate several systems: a financial exchange app called Liquibook, Redis, Memcached, and HERD [33]. Our evaluation shows that Mu incurs a small replication latency, in some cases being the only viable replication system that incurs an acceptable overhead.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>OSDI'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/287869">SafetyPin: Encrypted Backups with Human-Memorable Secrets</a>.
			</span><br />
			<span class="authors">
				Emma Dauterman, Henry Corrigan-Gibbs, and David Mazieres.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Dauterman:287869" class="abstract">
			We present the design and implementation of SafetyPin, a system for encrypted mobile-device backups. Like existing cloud-based mobile-backup systems, including those of Apple and Google, SafetyPin requires users to remember only a short PIN and defends against brute-force PIN-guessing attacks using hardware security protections. Unlike today&#39;s systems, SafetyPin splits trust over a cluster of hardware security modules (HSMs) in order to provide security guarantees that scale with the number of HSMs. In this way, SafetyPin protects backed-up user data even against an attacker that can adaptively compromise many of the system&#39;s constituent HSMs. SafetyPin provides this protection without sacrificing scalability or fault tolerance. Decentralizing trust while respecting the resource limits of today&#39;s HSMs requires a synthesis of systems-design principles and cryptographic tools. We evaluate SafetyPin on a cluster of 100 low-cost HSMs and show that a SafetyPin-protected recovery takes 1.01 seconds. To process 1B recoveries a venue_short, we estimate that a SafetyPin deployment would need 3,100 low-cost HSMs.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>OSDI'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/282923">A Simpler and Faster NIC Driver Model for Network Functions</a>.
			</span><br />
			<span class="authors">
				Solal Pirelli and George Candea.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Pirelli:282923" class="abstract">
			The advent of software network functions calls for stronger correctness guarantees and higher performance at every level of the stack. Current network stacks trade simplicity for performance and flexibility, especially in their driver model. We show that performance and simplicity can coexist, at the cost of some flexibility, with a new NIC driver model tailored to network functions. The key idea behind our model is that the driver can efficiently reuse packet buffers because buffers follow a single logical path. We implement a driver for the Intel 82599 network card in 550 lines of code. By merely replacing the state-of-theart driver with our driver, formal verification of the entire software stack completes in 7x less time, while the verified functions‚Äô throughput improves by 160%. Our driver also beats, on realistic workloads, the throughput of drivers that cannot yet be formally verified, thanks to its low variability and resource use. Our code is available at github.com/dslab-epfl/tinynf.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>Usenix Security'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/287859">DELF: Safeguarding Deletion Correctness in Online Social Networks</a>.
			</span><br />
			<span class="authors">
				Katriel Cohn-Gordon, Georgios Damaskinos, Divino Neto, Shi Cordova, Benoit Reitz, Benjamin Strahs, Daniel Obenshain, Paul Pearce, and Loannis Papagiannis.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Cohn-Gordon:287859" class="abstract">
			Deletion is a core facet of Online Social Networks (OSNs). For users, deletion is a tool to remove what they have shared and control their data. For OSNs, robust deletion is both an obligation to their users and a risk when developer mistakes inevitably occur. While developers are effective at identifying high-level deletion requirements in products (e.g., users should be able to delete posted photos), they are less effective at mapping high-level requirements into concrete operations (e.g., deleting all relevant items in data stores). Without framework support, developer mistakes lead to violations of users&#39; privacy, such as retaining data that should be deleted, deleting the wrong data, and exploitable vulnerabilities.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>Usenix Security'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/287822">FuzzGen: Automatic Fuzzer Generation</a>.
			</span><br />
			<span class="authors">
				Kyriakos K. Ispoglou, Daniel Austin, Vishwath Mohan, and Mathias Payer.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Ispoglou:287822" class="abstract">
			Fuzzing is a testing technique to discover unknown vulnerabilities in software. When applying fuzzing to libraries, the core idea of supplying random input remains unchanged, yet it is non-trivial to achieve good code coverage. Libraries cannot run as standalone programs, but instead are invoked through another application. Triggering code deep in a library remains challenging as specific sequences of API calls are required to build up the necessary state. Libraries are diverse and have unique interfaces that require unique fuzzers, so far written by a human analyst.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>Usenix Security'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/287890">HALucinator: Firmware Re-hosting Through Abstraction Layer Emulation</a>.
			</span><br />
			<span class="authors">
				Abraham A. Clements, Eric Gustafson, Tobias Scharnowski, Paul Grosen, David Fritz, Christopher Kruegel, Giovanni Vigna, Saurabh Bagchi, and Mathias Payer.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Clements:287890" class="abstract">
			Given the increasing ubiquity of online embedded devices, analyzing their firmware is important to security, privacy, and safety. The tight coupling between hardware and firmware and the diversity found in embedded systems makes it hard to perform dynamic analysis on firmware. However, firmware developers regularly develop code using abstractions, such as Hardware Abstraction Layers (HALs), to simplify their job. We leverage such abstractions as the basis for the re-hosting and analysis of firmware. By providing high-level replacements for HAL functions (a process termed High-Level Emulation - HLE), we decouple the hardware from the firmware. This approach works by first locating the library functions in a firmware sample, through binary analysis, and then providing generic implementations of these functions in a full-system emulator.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>Usenix Security'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/287847">USBFuzz: A Framework for Fuzzing USB Drivers by Device Emulation</a>.
			</span><br />
			<span class="authors">
				Hui Peng and Mathias Payer.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Peng:287847" class="abstract">
			The Universal Serial Bus (USB) connects external devices to a host. This interface exposes the OS kernels and device drivers to attacks by malicious devices. Unfortunately, kernels and drivers were developed under a security model that implicitly trusts connected devices. Drivers expect faulty hardware but not malicious attacks. Similarly, security testing drivers is challenging as input must cross the hardware/software barrier. Fuzzing, the most widely used bug finding technique, relies on providing random data to programs. However, fuzzing device drivers is challenging due to the difficulty in crossing the hardware/software barrier and providing random device data to the driver under test.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>VLDB End.'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/282822">A System Design for Elastically Scaling Transaction Processing Engines in Virtualized Servers</a>.
			</span><br />
			<span class="authors">
				Angelos-Christos Anadiotis, Raja Appuswamy, Anastasia Ailamaki, Ilan Bronshtein, Hillel Avni, David Dominguez-Sal, Shay Goikhman, and Eliezer Levy.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Anadiotis:282822" class="abstract">
			Online Transaction Processing (OLTP) deployments are migrating from on-premise to cloud settings in order to exploit the elasticity of cloud infrastructure which allows them to adapt to workload variations. However, cloud adaptation comes at the cost of redesigning the engine, which has led to the introduction of several, new, cloud-based transaction processing systems mainly focusing on: (i) the transaction coordination protocol, (ii) the data partitioning strategy, and, (iii) the resource isolation across multiple tenants. As a result, standalone OLTP engines cannot be easily deployed with an elastic setting in the cloud and they need to migrate to another, specialized deployment.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>VLDB End.'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/280879">Micro-architectural Analysis of OLAP: Limitations and Opportunities</a>.
			</span><br />
			<span class="authors">
				Utku Sirin and Anastasia Ailamaki.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Sirin:280879" class="abstract">
			Understanding micro-architectural behavior is important for efficiently using hardware resources. Recent work has shown that in-memory online transaction processing (OLTP) systems severely underutilize their core micro-architecture resources [29]. Whereas, online analytical processing (OLAP) workloads exhibit a completely different computing pattern. OLAP workloads are read-only, bandwidth-intensive, and include various data access patterns. With the rise of column-stores, they run on high-performance engines that are tightly optimized for modern hardware. Consequently, micro-architectural behavior of modern OLAP systems remains unclear.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>VLDB'20</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/275745">Adaptive Partitioning and Indexing for In-situ Query Processing</a>.
			</span><br />
			<span class="authors">
				Matthaios Olma, Manos Karpathiotakis, Ioannis Alagiannis, Manos Athanassoulis, and Anastasia Ailamaki.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Olma:275745" class="abstract">
			The constant flux of data and queries alike has been pushing the boundaries of data analysis systems. The increasing size of raw data files has made data loading an expensive operation that delays the data-to-insight time. To alleviate the loading cost, in situ query processing systems operate directly over raw data and offer instant access to data. At the same time, analytical workloads have increasing number of queries. Typically, each query focuses on a constantly shifting-yet small-range. As a result, minimizing the workload latency requires the benefits of indexing in in situ query processing. In this paper, we present an online partitioning and indexing scheme, along with a partitioning and indexing tuner tailored for in situ querying engines. The proposed system design improves query execution time by taking into account user query patterns, to (i) partition raw data files logically and (ii) build lightweight partition-specific indexes for each partition. We build an in situ query engine called Slalom to showcase the impact of our design. Slalom employs adaptive partitioning and builds non-obtrusive indexes in different partitions on-the-fly based on lightweight query access pattern monitoring. As a result of its lightweight nature, Slalom achieves efficient query processing over raw data with minimal memory consumption. Our experimentation with both microbenchmarks and real-life workloads shows that Slalom outperforms state-of-the-art in situ engines and achieves comparable query response times with fully indexed DBMS, offering lower cumulative query execution times for query workloads with increasing size and unpredictable access patterns.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>ASPLOS'19</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/265809">RPCValet: NI-Driven Tail-Aware Balancing of ¬µs-Scale RPCs</a>.
			</span><br />
			<span class="authors">
				Alexandros Daglis, Mark Sutherland, and Babak Falsafi.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Daglis:265809" class="abstract">
			Modern online services come with stringent quality requirements in terms of response time tail latency. Because of their decomposition into fine-grained communicating software layers, a single user request fans out into a plethora of short, Œºs-scale RPCs, aggravating the need for faster inter-server communication. In reaction to that need, we are witnessing a technological transition characterized by the emergence of hardware-terminated user-level protocols (e.g., InfiniBand/RDMA) and new architectures with fully integrated Network Interfaces (NIs). Such architectures offer a unique opportunity for a new NI-driven approach to balancing RPCs among the cores of manycore server CPUs, yielding major tail latency improvements for Œºs-scale RPCs. We introduce RPCValet, an NI-driven RPC load-balancing design for architectures with hardware-terminated protocols and integrated NIs, that delivers near-optimal tail latency. RPCValet&#39;s RPC dispatch decisions emulate the theoretically optimal single-queue system, without incurring synchronization overheads currently associated with single-queue implementations. Our design improves throughput under tight tail latency goals by up to 1.4x, and reduces tail latency before saturation by up to 4x for RPCs with Œºs-scale service times, as compared to current systems with hardware support for RPC load distribution. RPCValet performs within 15% of the theoretically optimal single-queue system.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>ISCA'19</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/276919">Linebacker: Preserving Victim Cache Lines in Idle Register Files of GPUs</a>.
			</span><br />
			<span class="authors">
				Yunho Oh, Gunjae Koo, Murali Annavaram, and Won Woo Ro.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Oh:276919" class="abstract">
			Modern GPUs suffer from cache contention due to the limited cache size that is shared across tens of concurrently running warps. To increase the per-warp cache size prior techniques proposed warp throttling which limits the number of active warps. Warp throttling leaves several registers to be dynamically unused whenever a warp is throttled. Given the stringent cache size limitation in GPUs this work proposes a new cache management technique named Linebacker (LB) that improves GPU performance by utilizing idle register file space as victim cache space. Whenever a CTA becomes inactive, linebacker backs up the registers of the throttled CTA to the off-chip memory. Then, linebacker utilizes the corresponding register file space as victim cache space. If any load instruction finds data in the victim cache line, the data is directly copied to the destination register through a simple register-register move operation. To further improve the efficiency of victim cache linebacker allocates victim cache space only to a select few load instructions that exhibit high data locality. Through a careful design of victim cache indexing and management scheme linebacker provides 29.0% of speedup compared to the previously proposed warp throttling techniques.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>MICRO'19</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/270293">Distributed Logless Atomic Durability with Persistent Memory</a>.
			</span><br />
			<span class="authors">
				Siddharth Gupta, Alexandros Daglis, and Babak Falsafi.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Gupta:270293" class="abstract">
			Datacenter operators have started deploying Persistent Memory (PM), leveraging its combination of fast access and persistence for significant performance gains. A key challenge for PM-aware software is to maintain high performance while achieving atomic durability. The latter typically requires the use of logging, which introduces considerable overhead with additional CPU cycles, write traffic, and ordering requirements. In this paper, we exploit the data multiversioning inherent in the memory hierarchy to achieve atomic durability without logging. Our design, LAD, relies on persistent buffering space at the memory controllers (MCs)‚Äîalready present in modern CPUs‚Äîto speculatively accumulate all of a transaction‚Äôs updates before they are all atomically committed to PM. LAD employs an on-chip distributed commit protocol in hardware to manage the distributed speculative state each transaction accumulates across multiple MCs. We demonstrate that LAD is a practical design relying on modest hardware modifications to provide atomically durable transactions, while delivering up to 80% of ideal‚Äîi.e., PM-oblivious software‚Äôs‚Äîperformance.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>MICRO'19</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/276507">Prefetched Address Translation</a>.
			</span><br />
			<span class="authors">
				Artemiy Margaritov, Dmitrii Ustiugov, Edouard Bugnion, and Boris Grot.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Margaritov:276507" class="abstract">
			With explosive growth in dataset sizes and increasing machine memory capacities, per-application memory footprints are commonly reaching into hundreds of GBs. Such huge datasets pressure the TLB, resulting in frequent misses that must be resolved through a page walk - a long-latency pointer chase through multiple levels of the in-memory radix tree-based page table.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>NSDI'19</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/264778">Performance Contracts for Software Network Functions</a>.
			</span><br />
			<span class="authors">
				Rishabh Ramesh Iyer, Luis David Pedrosa, Arseniy Zaostrovnykh, Solal Pirelli, Katerina Argyraki, and George Candea.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Iyer:264778" class="abstract">
			Software network functions (NFs), or middleboxes, promise flexibility and easy deployment of network services but face the serious challenge of unexpected performance behaviour. We propose the notion of a performance contract, a construct formulated in terms of performance critical variables, that provides a precise description of NF performance. Performance contracts enable fine-grained prediction and scrutiny of NF performance for arbitrary workloads, without having to run the NF itself. We describe BOLT, a technique and tool for computing such performance contracts for the entire software stack of NFs written in C, including the core NF logic, DPDK packet processing framework, and NIC driver. BOLT takes as input the NF implementation code and outputs the corresponding contract. Under the covers, it combines pre-analysis of a library of stateful NF data structures with automated symbolic execution of the NF‚Äôs code. We evaluate BOLT on four NFs‚Äîa Maglev-like load balancer, a NAT, an LPM router, and a MAC bridge‚Äîand show that its performance contracts predict the dynamic instruction count and memory access count with a maximum gap of 7% between the real execution and the conservatively predicted upper bound. With further engineering, this gap can be reduced.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>SIGMOD'19</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/273429">Fast General Distributed Transactions with Opacity</a>.
			</span><br />
			<span class="authors">
				Alex Shamis, Matthew Renzelmann, Stanko Novakovic, Georgios Chatzopoulos, Aleksandar Dragojevic, Dushyanth Narayanan, and Miguel Castro.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Shamis:273429" class="abstract">
			Transactions can simplify distributed applications by hiding data distribution, concurrency, and failures from the application developer. Ideally the developer would see the abstraction of a single large machine that runs transactions sequentially and never fails. This requires the transactional subsystem to provide opacity (strict serializability for both committed and aborted transactions), as well as transparent fault tolerance with high availability. As even the best abstractions are unlikely to be used if they perform poorly, the system must also provide high performance. Existing distributed transactional designs either weaken this abstraction or are not designed for the best performance within a data center. This paper extends the design of FaRM - which provides strict serializability only for committed transactions - to provide opacity while maintaining FaRM&#39;s high throughput, low latency, and high availability within a modern data center. It uses timestamp ordering based on real time with clocks synchronized to within tens of microseconds across a cluster, and a failover protocol to ensure correctness across clock master failures. FaRM with opacity can commit 5.4 million neworder transactions per second when running the TPC-C transaction mix on 90 machines with 3-way replication.
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>SOSP'19</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/275534">Verifying Software Network Functions with No Verification Expertise</a>.
			</span><br />
			<span class="authors">
				Arseniy Zastrovnykh, Solal Pirelli, Rishabh Iyer, Matteo Rizzo, Luis Pedrosa, Katerina Argyraki, and George Candea.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Zastrovnykh:275534" class="abstract">
			We present the design and implementation of Vigor, a software stack and toolchain for building and running software network middleboxes that are guaranteed to be correct, while preserving competitive performance and developer productivity. Developers write the core of the middlebox---the network function (NF)---in C, on top of a standard packet-processing framework, putting persistent state in data structures from Vigor&#39;s library; the Vigor toolchain then automatically verifies that the resulting software stack correctly implements a specification, which is written in Python. Vigor has three key features: network function developers need no verification expertise, and the verification process does not require their assistance (push-button verification); the entire software stack is verified, down to the hardware (full-stack verification); and verification can be done in a pay-as-you-go manner, i.e., instead of investing upfront a lot of time in writing and verifying a complete specification, one can specify one-off properties in a few lines of Python and verify them without concern for the rest. We developed five representative NFs---a NAT, a Maglev load balancer, a MAC-learning bridge, a firewall, and a traffic policer---and verified with Vigor that they satisfy standards-derived specifications, are memory-safe, and do not crash or hang. We show that they provide competitive performance. The Vigor framework is available at [http://vigor.epfl.ch].
			</div>
			</details>
		</td>
	</tr>
	<tr>
		<td>
			<span class="date">
				
				<big><strong>Usenix Security'19</strong></big><br />
				
			</span>
		</td>
		<td class="publication">
			<span class="pubtitle">
				<a href="http://infoscience.epfl.ch/record/275441">Pythia: Remote Oracles for the Masses</a>.
			</span><br />
			<span class="authors">
				Shin-Yeh Tsai, Mathias Payer, and Yiying Zhang.
			</span><br />
			<span class="links">
				<details><summary>Abstract</summary>
			</span>
			<div id="abstract_Tsai:275441" class="abstract">
			Remote Direct Memory Access (RDMA) is a technology that allows direct access from the network to a machine&#39;s main memory without involving its CPU. RDMA offers low-latency, high-bandwidth performance and low CPU utilization. While RDMA provides massive performance boosts and has thus been adopted by several major cloud providers, security concerns have so far been neglected.
			</div>
			</details>
		</td>
	</tr>
</tbody>
</table>
